{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import GPyOpt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KINIT_USERNAME = 'oalenkin'\n",
    "KINIT_PASSWD = 'Ubivator94'\n",
    "N_EVENTS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_dist = 3.6\n",
    "\n",
    "space = [{'name': 'pitch', 'type': 'continuous', 'domain': (min_dist, min_dist)},\\\n",
    "         {'name': 'yoffset_layer', 'type': 'continuous', 'domain': (min_dist/2, min_dist)},\\\n",
    "         {'name': 'yoffset_plane', 'type': 'continuous', 'domain': (min_dist*0.25, min_dist*1.25)},\\\n",
    "         {'name': 'zshift_layer', 'type': 'continuous', 'domain': (1.6, 2.6)},\\\n",
    "         {'name': 'zshift_plane', 'type': 'continuous', 'domain': (3.8, 6.8)},\\\n",
    "         {'name': 'zshift_view', 'type': 'continuous', 'domain': (10, 10)},\\\n",
    "         {'name': 'alpha', 'type': 'discrete', 'domain': (5, 5)}]\n",
    "\n",
    "constraints = [{'name': 'constr_1', 'constrain': '-(x[:,0]-x[:,1])**2-x[:,3]**2+2**2'},\\\n",
    "               {'name': 'constr_2', 'constrain': '-(x[:,1]-x[:,2])**2-(x[:,3]-x[:,4])**2+2**2'},\\\n",
    "               {'name': 'constr_3', 'constrain': 'x[:,3]+x[:,4]+2-x[:,5]'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feasible_region = GPyOpt.Design_space(space=space, constraints=constraints)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import skygrid client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "from disneylandClient import (\n",
    "    new_client,\n",
    "    Job,\n",
    "    RequestWithId,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STATUS_IN_PROCESS = set([\n",
    "    Job.PENDING,\n",
    "    Job.PULLED,\n",
    "    Job.RUNNING,\n",
    "])\n",
    "STATUS_FINAL = set([\n",
    "    Job.COMPLETED,\n",
    "    Job.FAILED,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_descriptor(point):\n",
    "    \n",
    "    pitch, yoffset_layer, yoffset_plane, zshift_layer, zshift_plane, zshift_view, alpha = point\n",
    "    \n",
    "    logining = \"sh -lc 'echo \"+KINIT_PASSWD+\" | kinit \"+KINIT_USERNAME+\"; \"\n",
    "    sourcing = \"source /opt/FairShipRun/config.sh; \"\n",
    "    simulation = \"python $SHIPOPT/code/objective.py --pitch \"+str(pitch)+\" --yoffset_layer \"+str(yoffset_layer)+\\\n",
    "              \" --yoffset_plane \"+str(yoffset_plane)+\" --zshift_layer \"+str(zshift_layer)+\" --zshift_plane \"+\\\n",
    "              str(zshift_plane)+\" --zshift_view \"+str(zshift_view)+\" --alpha \"+str(int(alpha))+\\\n",
    "              \" --nEvents \"+str(N_EVENTS)+\" --output /output/output.txt'\"\n",
    "    cmd = logining+sourcing+simulation\n",
    "\n",
    "    descriptor = {\n",
    "        \"input\": [],\n",
    "\n",
    "        \"container\": {\n",
    "            \"workdir\": \"\",\n",
    "            \"name\": \"oleg94/worker_layer\",\n",
    "            \"cpu_needed\": 1,\n",
    "            \"max_memoryMB\": 4096,\n",
    "            \"min_memoryMB\": 1024,\n",
    "            \"cmd\": cmd,\n",
    "        },\n",
    "\n",
    "        \"required_outputs\": {\n",
    "            \"output_uri\": \"none:\",\n",
    "            \"file_contents\": [\n",
    "                {\"file\": \"output.txt\", \"to_variable\": \"out\"}\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = 100\n",
    "n_initial_design = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_design = GPyOpt.experiment_design.initial_design('random', feasible_region, n_initial_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_objective = np.zeros(n_initial_design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stub = new_client()\n",
    "jobs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #0 started.\n",
      "0  pushed\n",
      "1  pushed\n",
      "2  pushed\n",
      "3  pushed\n",
      "4  pushed\n",
      "5  pushed\n",
      "6  pushed\n",
      "7  pushed\n",
      "8  pushed\n",
      "9  pushed\n",
      "10  pushed\n",
      "11  pushed\n",
      "12  pushed\n",
      "13  pushed\n",
      "14  pushed\n",
      "15  pushed\n",
      "16  pushed\n",
      "17  pushed\n",
      "18  pushed\n",
      "19  pushed\n",
      "20  pushed\n",
      "21  pushed\n",
      "22  pushed\n",
      "23  pushed\n",
      "24  pushed\n",
      "25  pushed\n",
      "26  pushed\n",
      "27  pushed\n",
      "28  pushed\n",
      "29  pushed\n",
      "30  pushed\n",
      "31  pushed\n",
      "32  pushed\n",
      "33  pushed\n",
      "34  pushed\n",
      "35  pushed\n",
      "36  pushed\n",
      "37  pushed\n",
      "38  pushed\n",
      "39  pushed\n",
      "40  pushed\n",
      "41  pushed\n",
      "42  pushed\n",
      "43  pushed\n",
      "44  pushed\n",
      "45  pushed\n",
      "46  pushed\n",
      "47  pushed\n",
      "48  pushed\n",
      "49  pushed\n",
      "50  pushed\n",
      "51  pushed\n",
      "52  pushed\n",
      "53  pushed\n",
      "54  pushed\n",
      "55  pushed\n",
      "56  pushed\n",
      "57  pushed\n",
      "58  pushed\n",
      "59  pushed\n",
      "60  pushed\n",
      "61  pushed\n",
      "62  pushed\n",
      "63  pushed\n",
      "64  pushed\n",
      "65  pushed\n",
      "66  pushed\n",
      "67  pushed\n",
      "68  pushed\n",
      "69  pushed\n",
      "70  pushed\n",
      "71  pushed\n",
      "72  pushed\n",
      "73  pushed\n",
      "74  pushed\n",
      "75  pushed\n",
      "76  pushed\n",
      "77  pushed\n",
      "78  pushed\n",
      "79  pushed\n",
      "80  pushed\n",
      "81  pushed\n",
      "82  pushed\n",
      "83  pushed\n",
      "84  pushed\n",
      "85  pushed\n",
      "86  pushed\n",
      "87  pushed\n",
      "88  pushed\n",
      "89  pushed\n",
      "90  pushed\n",
      "91  pushed\n",
      "92  pushed\n",
      "93  pushed\n",
      "94  pushed\n",
      "95  pushed\n",
      "96  pushed\n",
      "97  pushed\n",
      "98  pushed\n",
      "99  pushed\n",
      "Finished jobs: 0 Running jobs: 56 Pending jobs: 44\n",
      "Finished jobs: 0 Running jobs: 100 Pending jobs: 0\n",
      "Finished jobs: 1 Running jobs: 99 Pending jobs: 0\n",
      "Finished jobs: 2 Running jobs: 98 Pending jobs: 0\n",
      "Finished jobs: 4 Running jobs: 96 Pending jobs: 0\n",
      "Finished jobs: 10 Running jobs: 90 Pending jobs: 0\n",
      "Finished jobs: 22 Running jobs: 78 Pending jobs: 0\n",
      "Finished jobs: 55 Running jobs: 45 Pending jobs: 0\n",
      "Finished jobs: 69 Running jobs: 31 Pending jobs: 0\n",
      "Finished jobs: 82 Running jobs: 18 Pending jobs: 0\n",
      "Finished jobs: 90 Running jobs: 10 Pending jobs: 0\n",
      "Finished jobs: 91 Running jobs: 9 Pending jobs: 0\n",
      "Finished jobs: 92 Running jobs: 8 Pending jobs: 0\n",
      "Finished jobs: 93 Running jobs: 7 Pending jobs: 0\n",
      "Finished jobs: 94 Running jobs: 6 Pending jobs: 0\n",
      "Finished jobs: 95 Running jobs: 5 Pending jobs: 0\n",
      "Finished jobs: 96 Running jobs: 4 Pending jobs: 0\n",
      "Finished jobs: 97 Running jobs: 3 Pending jobs: 0\n",
      "Finished jobs: 98 Running jobs: 2 Pending jobs: 0\n",
      "Finished jobs: 100 Running jobs: 0 Pending jobs: 0\n"
     ]
    }
   ],
   "source": [
    "init_d_i = 0\n",
    "\n",
    "for epoch in range(n_initial_design // n_estimators):\n",
    "    \n",
    "    print(\"EPOCH #\"+str(epoch)+\" started.\")\n",
    "    \n",
    "    epoch_jobs = [0] * n_estimators\n",
    "    \n",
    "    for k in range(n_estimators):\n",
    "        descriptor = return_descriptor(initial_design[init_d_i])\n",
    "        init_d_i += 1\n",
    "        epoch_jobs[k] = Job(input=json.dumps(descriptor), kind=\"docker\")\n",
    "        epoch_jobs[k] = stub.CreateJob(epoch_jobs[k])\n",
    "        print(k, \" pushed\")\n",
    "    \n",
    "    prev_number_of_finished_jobs = 0\n",
    "    prev_number_of_running_jobs = 0\n",
    "    prev_number_of_pending_jobs = 0\n",
    "    \n",
    "    while True:\n",
    "        for k in range(n_estimators):\n",
    "            epoch_jobs[k] = stub.GetJob(RequestWithId(id=epoch_jobs[k].id))\n",
    "        \n",
    "        number_of_finished_jobs = 0\n",
    "        number_of_running_jobs = 0\n",
    "        number_of_pending_jobs = 0\n",
    "        for k in range(n_estimators):\n",
    "            if epoch_jobs[k].status in STATUS_FINAL:\n",
    "                number_of_finished_jobs += 1\n",
    "            if epoch_jobs[k].status == Job.PENDING:\n",
    "                number_of_pending_jobs += 1\n",
    "            if epoch_jobs[k].status == Job.RUNNING:\n",
    "                number_of_running_jobs += 1\n",
    "                \n",
    "        if (number_of_finished_jobs != prev_number_of_finished_jobs) or (prev_number_of_running_jobs != number_of_running_jobs) or (prev_number_of_pending_jobs != number_of_pending_jobs):\n",
    "            print(\"Finished jobs: \"+str(number_of_finished_jobs)+\\\n",
    "                  \" Running jobs: \"+str(number_of_running_jobs)+\\\n",
    "                  \" Pending jobs: \"+str(number_of_pending_jobs))\n",
    "            prev_number_of_finished_jobs = number_of_finished_jobs\n",
    "            prev_number_of_running_jobs = number_of_running_jobs\n",
    "            prev_number_of_pending_jobs = number_of_pending_jobs\n",
    "            \n",
    "        if number_of_finished_jobs == n_estimators:\n",
    "            break\n",
    "        time.sleep(120)\n",
    "    \n",
    "    jobs += epoch_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_init_design = pd.DataFrame(initial_design, columns=['pitch', 'yoffset_layer', 'yoffset_plane', 'zshift_layer', 'zshift_plane', 'zshift_view', 'alpha'])\n",
    "df_init_design['objective'] = [float(re.sub('[\\[\\]\"variable:out=]', '', job.output)) if re.sub('[\\[\\]\"variable:out=]', '', job.output) else np.nan for job in jobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_init_design.to_csv('../observations/observations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main part of optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_init_design = pd.read_csv('../observations/observations.csv')\n",
    "n_estimators = 60\n",
    "n_epochs = 500\n",
    "stub = new_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH #0 started.\n",
      "Finished jobs: 0 Running jobs: 33 Pending jobs: 25\n",
      "Finished jobs: 0 Running jobs: 60 Pending jobs: 0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "        print(\"EPOCH #\"+str(epoch)+\" started.\")\n",
    "    \n",
    "#         epoch_jobs = [0] * n_estimators\n",
    "        \n",
    "#         pending_X = []\n",
    "\n",
    "#         for k in range(n_estimators):\n",
    "            \n",
    "#             step_X = df_init_design[df_init_design.columns[:-1]].values\n",
    "#             #because we want to maximize\n",
    "#             step_Y = -df_init_design[df_init_design.columns[-1:]].values\n",
    "#             ignored_X = step_X[np.isnan(step_Y.ravel())]\n",
    "#             step_X = step_X[~np.isnan(step_Y.ravel())]\n",
    "#             step_Y = step_Y[~np.isnan(step_Y.ravel())]\n",
    "#             bo = GPyOpt.methods.BayesianOptimization(f=None, domain=space, constraints=constraints, X=step_X,\\\n",
    "#                                                      Y=step_Y, initial_design_numdata=100,\\\n",
    "#                                                      evaluator_type='local_penalization', batch_size=n_estimators)\n",
    "#             new_point = bo.suggest_next_locations(pending_X=np.array(pending_X), ignored_X=ignored_X)[0]\n",
    "\n",
    "#             descriptor = return_descriptor(new_point)\n",
    "#             epoch_jobs[k] = Job(input=json.dumps(descriptor), kind=\"docker\")\n",
    "#             epoch_jobs[k] = stub.CreateJob(epoch_jobs[k])\n",
    "            \n",
    "#             pending_X += [new_point]\n",
    "\n",
    "\n",
    "        step_X = df_init_design[df_init_design.columns[:-1]].values\n",
    "        #because we want to maximize\n",
    "        step_Y = -df_init_design[df_init_design.columns[-1:]].values\n",
    "        ignored_X = step_X[np.isnan(step_Y.ravel())]\n",
    "        step_X = step_X[~np.isnan(step_Y.ravel())]\n",
    "        step_Y = step_Y[~np.isnan(step_Y.ravel())]\n",
    "        bo = GPyOpt.methods.BayesianOptimization(f=None, domain=space, constraints=constraints, X=step_X,\\\n",
    "                                                 Y=step_Y, initial_design_numdata=100,\\\n",
    "                                                 evaluator_type='local_penalization', batch_size=n_estimators)\n",
    "        \n",
    "        pending_X = list(bo.suggest_next_locations(ignored_X=ignored_X))\n",
    "        \n",
    "        epoch_jobs = [0] * len(pending_X)\n",
    "        for k, new_point in enumerate(pending_X):\n",
    "            descriptor = return_descriptor(new_point)\n",
    "            epoch_jobs[k] = Job(input=json.dumps(descriptor), kind=\"docker\")\n",
    "            epoch_jobs[k] = stub.CreateJob(epoch_jobs[k])\n",
    "        \n",
    "        prev_number_of_finished_jobs = 0\n",
    "        prev_number_of_running_jobs = 0\n",
    "        prev_number_of_pending_jobs = 0\n",
    "\n",
    "        while True:\n",
    "            for k in range(n_estimators):\n",
    "                epoch_jobs[k] = stub.GetJob(RequestWithId(id=epoch_jobs[k].id))\n",
    "\n",
    "            number_of_finished_jobs = 0\n",
    "            number_of_running_jobs = 0\n",
    "            number_of_pending_jobs = 0\n",
    "            for k in range(n_estimators):\n",
    "                if epoch_jobs[k].status in STATUS_FINAL:\n",
    "                    number_of_finished_jobs += 1\n",
    "                if epoch_jobs[k].status == Job.PENDING:\n",
    "                    number_of_pending_jobs += 1\n",
    "                if epoch_jobs[k].status == Job.RUNNING:\n",
    "                    number_of_running_jobs += 1\n",
    "\n",
    "            if (number_of_finished_jobs != prev_number_of_finished_jobs) or (prev_number_of_running_jobs != number_of_running_jobs) or (prev_number_of_pending_jobs != number_of_pending_jobs):\n",
    "                print(\"Finished jobs: \"+str(number_of_finished_jobs)+\\\n",
    "                      \" Running jobs: \"+str(number_of_running_jobs)+\\\n",
    "                      \" Pending jobs: \"+str(number_of_pending_jobs))\n",
    "                prev_number_of_finished_jobs = number_of_finished_jobs\n",
    "                prev_number_of_running_jobs = number_of_running_jobs\n",
    "                prev_number_of_pending_jobs = number_of_pending_jobs\n",
    "\n",
    "            if number_of_finished_jobs == n_estimators:\n",
    "                break\n",
    "            time.sleep(120)\n",
    "            \n",
    "        for k, point in enumerate(pending_X):\n",
    "        \n",
    "            value = float(re.sub('[\\[\\]\"variable:out=]', '', epoch_jobs[k].output)) if re.sub('[\\[\\]\"variable:out=]', '', epoch_jobs[k].output) else np.nan\n",
    "        \n",
    "            df_init_design.loc[len(df_init_design)] = list(point)+[value]\n",
    "            df_init_design.to_csv('../observations/observations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished jobs: 42 Running jobs: 18 Pending jobs: 0\n"
     ]
    }
   ],
   "source": [
    "for k in range(n_estimators):\n",
    "    epoch_jobs[k] = stub.GetJob(RequestWithId(id=epoch_jobs[k].id))\n",
    "\n",
    "number_of_finished_jobs = 0\n",
    "number_of_running_jobs = 0\n",
    "number_of_pending_jobs = 0\n",
    "for k in range(n_estimators):\n",
    "    if epoch_jobs[k].status in STATUS_FINAL:\n",
    "        number_of_finished_jobs += 1\n",
    "    if epoch_jobs[k].status == Job.PENDING:\n",
    "        number_of_pending_jobs += 1\n",
    "    if epoch_jobs[k].status == Job.RUNNING:\n",
    "        number_of_running_jobs += 1\n",
    "\n",
    "\n",
    "print(\"Finished jobs: \"+str(number_of_finished_jobs)+\\\n",
    "      \" Running jobs: \"+str(number_of_running_jobs)+\\\n",
    "      \" Pending jobs: \"+str(number_of_pending_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, point in enumerate(pending_X):\n",
    "        \n",
    "    value = float(re.sub('[\\[\\]\"variable:out=]', '', epoch_jobs[k].output)) if re.sub('[\\[\\]\"variable:out=]', '', epoch_jobs[k].output) else np.nan\n",
    "    df_init_design.loc[len(df_init_design)] = list(point)+[value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_init_design[~np.isnan(df_init_design.objective)].to_csv('../observations/observations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
